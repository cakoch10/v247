---
title: 'Bridging the Gap: Rademacher Complexity in Robust and Standard Generalization'
section: Original Papers
abstract: 'Training Deep Neural Networks (DNNs) with adversarial examples often results
  in poor generalization to test-time adversarial data. This paper investigates this
  issue, known as adversarially robust generalization, through the lens of Rademacher
  complexity. Building upon the studies by Khim and Loh (2018); Yin et al. (2019),
  numerous works have been dedicated to this problem, yet achieving a satisfactory
  bound remains an elusive goal. Existing works on DNNs either apply to a surrogate
  loss instead of the robust loss or yield bounds that are notably looser compared
  to their standard counterparts. In the latter case, the bounds have a higher dependency
  on the width $m$ of the DNNs or the dimension $d$ of the data, with an extra factor
  of at least $\mathcal{O}(\sqrt{m})$ or $\mathcal{O}(\sqrt{d})$. This paper presents
  upper bounds for adversarial Rademacher complexity of DNNs that match the best-known
  upper bounds in standard settings, as established in the work  of Bartlett et al.
  (2017), with the dependency on width and dimension being $\mathcal{O}(\ln(dm))$.
  The central challenge addressed is calculating the covering number of adversarial
  function classes. We aim to construct a new cover that possesses two properties:
  1) compatibility with adversarial examples, and 2) precision comparable to covers
  used in standard settings. To this end, we introduce a new variant of covering number
  called the \emph{uniform covering number}, specifically designed and proven to reconcile
  these two properties. Consequently, our method effectively bridges the gap between
  Rademacher complexity in robust and standard generalization.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: xiao24a
month: 0
tex_title: 'Bridging the Gap: Rademacher Complexity in Robust and Standard Generalization'
firstpage: 5074
lastpage: 5075
page: 5074-5075
order: 5074
cycles: false
bibtex_author: Xiao, Jiancong and Sun, Ruoyu and Long, Qi and Su, Weijie
author:
- given: Jiancong
  family: Xiao
- given: Ruoyu
  family: Sun
- given: Qi
  family: Long
- given: Weijie
  family: Su
date: 2024-06-30
address:
container-title: Proceedings of Thirty Seventh Conference on Learning Theory
volume: '247'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 6
  - 30
pdf: https://proceedings.mlr.press/v247/xiao24a/xiao24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
